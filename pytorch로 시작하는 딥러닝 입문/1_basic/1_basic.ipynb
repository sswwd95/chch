{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 1.torch\r\n",
    "tensor등의 다양한 수학 함수가 포함되어 있으며 numpy와 유사한 구조를 가진다. \r\n",
    "\r\n",
    "## 2.torch.autograd\r\n",
    "자동 미분을 위한 함수들이 포함되어 있다. \r\n",
    "\r\n",
    "## 3.torch.nn\r\n",
    "신경망을 구축하기 위한 다양한 데이터 구조나 레이어 등이 정의되어져 있다. \r\n",
    "\r\n",
    "## 4.torch.optim\r\n",
    "SGD를 중심으로 한 파라미터 최적화 알고리즘의 구현되어 있다.\r\n",
    "\r\n",
    "## 5.torch.utils.data\r\n",
    "SGD의 반복 연산을 실행할 때 사용하는 미니 배치용 유틸리티 함수가 포함되어 있다. \r\n",
    "\r\n",
    "## 6.torch.onnx\r\n",
    "ONNX(Open Neural Network Exchange)의 포맷으로 모델을 익스포트(export)할 때 사용한다. ONNX는 서로 다른 딥 러닝 프레임워크 간에 모델을 공유할 때 사용하는 포맷이다.\r\n",
    "\r\n",
    "---\r\n",
    "# 텐서 조작하기 1\r\n",
    "  ## 벡터, 행렬, 텐서\r\n",
    "    *스칼라 : 차원이 없는 값\r\n",
    "    *벡터 : 1차원으로 구성된 값\r\n",
    "    *행렬 : 2차원으로 구성된 값\r\n",
    "    *텐서 : 3차원 이상\r\n",
    "  ### 자연어 처리는 보통(batch size, 문장 길이, 단어 벡터의 차원)이라는 3차원 텐서를 사용한다.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "# 넘파이로 텐서 만들기(벡터와 행렬 만들기)\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "# 1D \r\n",
    "t = np.array([0,1,2,3,4,5,6])\r\n",
    "print(t)\r\n",
    "print(t.ndim)\r\n",
    "print(t.shape)\r\n",
    "print(t[0],t[1],t[-1])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0 1 2 3 4 5 6]\n",
      "1\n",
      "(7,)\n",
      "0 1 6\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "# 2D\r\n",
    "t = np.array([[1,2,3],[4,5,6],[7,8,9],[10,11,12]])\r\n",
    "print(t)\r\n",
    "print(t.ndim)\r\n",
    "print(t.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[ 1  2  3]\n",
      " [ 4  5  6]\n",
      " [ 7  8  9]\n",
      " [10 11 12]]\n",
      "2\n",
      "(4, 3)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "import torch\r\n",
    "\r\n",
    "# 1D\r\n",
    "t = torch.FloatTensor([0,1,2,3,4,5,6])\r\n",
    "print(t)\r\n",
    "print(t.dim()) # 차원\r\n",
    "print(t.size()) # shape\r\n",
    "print(t.shape) # shape \r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([0., 1., 2., 3., 4., 5., 6.])\n",
      "1\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "# 2D\r\n",
    "t = torch.FloatTensor([[1,2,3],[4,5,6],[7,8,9],[10,11,12]])\r\n",
    "print(t)\r\n",
    "print(t.dim())\r\n",
    "print(t.size())\r\n",
    "print(t[:,1])  # 첫번째 차원을 전체 선택한 상황에서 두번째 차원의 첫번째 것만 가져온다.\r\n",
    "print(t[:, :-1]) # 첫번째 차원을 전체 선택한 상황에서 두번째 차원에서는 맨 마지막에서 첫번째를 제외하고 다 가져온다."
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[ 1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.],\n",
      "        [ 7.,  8.,  9.],\n",
      "        [10., 11., 12.]])\n",
      "2\n",
      "torch.Size([4, 3])\n",
      "tensor([ 2.,  5.,  8., 11.])\n",
      "tensor([[ 1.,  2.],\n",
      "        [ 4.,  5.],\n",
      "        [ 7.,  8.],\n",
      "        [10., 11.]])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "# 브로드캐스팅(broadcasting)\r\n",
    "\r\n",
    "# 두 행렬이 곱셈을 할 때는 a의 마지막 차원과 b의 첫번째 차원이 일치해야한다.\r\n",
    "# 파이토치에서는 자동으로 크기를 맞춰서 연산을 수행하게 하는 브로드캐스팅 기능을 제공한다.\r\n",
    "\r\n",
    "m1 = torch.FloatTensor([[3,3]]) \r\n",
    "m2 = torch.FloatTensor([[2,2]])\r\n",
    "\r\n",
    "print(m1+m2)\r\n",
    "\r\n",
    "# vector + scalar\r\n",
    "m3 = torch.FloatTensor([[1,2]]) \r\n",
    "m4 = torch.FloatTensor([3])\r\n",
    "\r\n",
    "print(m3+m4)\r\n",
    "\r\n",
    "# 2 x 1 vector + 1 x 2 vector\r\n",
    "m5 = torch.FloatTensor([[1,2]]) \r\n",
    "m6 = torch.FloatTensor([[3],[4]])\r\n",
    "\r\n",
    "print(m5+m6)\r\n",
    "\r\n",
    "\r\n",
    "# 브로드캐스팅은 자동으로 수행되므로 사용자는 나중에 원하는 결과가 나오지 않았더라도 \r\n",
    "# 어디서 문제가 발생했는지 찾기가 굉장히 어려울 수 있다."
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[5., 5.]])\n",
      "tensor([[4., 5.]])\n",
      "tensor([[4., 5.],\n",
      "        [5., 6.]])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "\r\n",
    "# 행렬 곱셈 : matmul()\r\n",
    "m1 = torch.FloatTensor([[1, 2], [3, 4]])\r\n",
    "m2 = torch.FloatTensor([[1], [2]])\r\n",
    "print('Shape of Matrix 1: ', m1.shape) # 2 x 2\r\n",
    "print('Shape of Matrix 2: ', m2.shape) # 2 x 1\r\n",
    "print(m1.matmul(m2))\r\n",
    "\r\n",
    "# 원소 곱셈 :  mul() 또는 *\r\n",
    "m1 = torch.FloatTensor([[1, 2], [3, 4]])\r\n",
    "m2 = torch.FloatTensor([[1], [2]])\r\n",
    "print('Shape of Matrix 1: ', m1.shape) # 2 x 2\r\n",
    "print('Shape of Matrix 2: ', m2.shape) # 2 x 1\r\n",
    "print(m1 * m2) # 2 x 2\r\n",
    "print(m1.mul(m2))\r\n",
    "\r\n",
    "# 평균 : mean()\r\n",
    "t = torch.FloatTensor([1, 2])\r\n",
    "print(t.mean())\r\n",
    "\r\n",
    "# dim = 0 : 행을 지우고 열만 남기는 것 (첫 번째 차원 의미)\r\n",
    "# dim = 1 : 열을 지우고 행만 남기는 것 == dim = -1\r\n",
    "\r\n",
    "# 덧셈 : sum()\r\n",
    "\r\n",
    "# 최대(Max)는 원소의 최대값을 리턴하고, 아그맥스(ArgMax)는 최대값을 가진 인덱스를 리턴\r\n",
    "t = torch.FloatTensor([[1, 2], [3, 4]])\r\n",
    "print(t.max(dim=0))  \r\n",
    "\r\n",
    "# 3과 4의 인덱스는 [1, 1]\r\n",
    "\r\n",
    "# max 또는 argmax만 리턴받고 싶다면 다음과 같이 리턴값에도 인덱스를 부여\r\n",
    "print('Max: ', t.max(dim=0)[0])\r\n",
    "print('Argmax: ', t.max(dim=0)[1])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Shape of Matrix 1:  torch.Size([2, 2])\n",
      "Shape of Matrix 2:  torch.Size([2, 1])\n",
      "tensor([[ 5.],\n",
      "        [11.]])\n",
      "Shape of Matrix 1:  torch.Size([2, 2])\n",
      "Shape of Matrix 2:  torch.Size([2, 1])\n",
      "tensor([[1., 2.],\n",
      "        [6., 8.]])\n",
      "tensor([[1., 2.],\n",
      "        [6., 8.]])\n",
      "tensor(1.5000)\n",
      "torch.return_types.max(\n",
      "values=tensor([3., 4.]),\n",
      "indices=tensor([1, 1]))\n",
      "Max:  tensor([3., 4.])\n",
      "Argmax:  tensor([1, 1])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "# 뷰(View) - 원소의 수를 유지하면서 텐서의 크기 변경 == reshape\r\n",
    "t = np.array([[[0, 1, 2],\r\n",
    "               [3, 4, 5]],\r\n",
    "              [[6, 7, 8],\r\n",
    "               [9, 10, 11]]])\r\n",
    "ft = torch.FloatTensor(t)\r\n",
    "print(ft.shape)\r\n",
    "\r\n",
    "print(ft.view([-1, 3])) # ft라는 텐서를 (?, 3)의 크기로 변경\r\n",
    "print(ft.view([-1, 3]).shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([2, 2, 3])\n",
      "tensor([[ 0.,  1.,  2.],\n",
      "        [ 3.,  4.,  5.],\n",
      "        [ 6.,  7.,  8.],\n",
      "        [ 9., 10., 11.]])\n",
      "torch.Size([4, 3])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "# 스퀴즈(Squeeze) - 1인 차원을 제거\r\n",
    "ft = torch.FloatTensor([[0], [1], [2]])\r\n",
    "print(ft)\r\n",
    "print(ft.shape)\r\n",
    "\r\n",
    "print(ft.squeeze())\r\n",
    "print(ft.squeeze().shape)\r\n",
    "\r\n",
    "# 언스퀴즈(Unsqueeze) - 특정 위치에 1인 차원을 추가\r\n",
    "ft = torch.Tensor([0, 1, 2])\r\n",
    "print(ft.shape)\r\n",
    "\r\n",
    "print(ft.unsqueeze(0)) \r\n",
    "print(ft.unsqueeze(0).shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [2.]])\n",
      "torch.Size([3, 1])\n",
      "tensor([0., 1., 2.])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "tensor([[0., 1., 2.]])\n",
      "torch.Size([1, 3])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "# concatenate : torch.cat([ ])\r\n",
    "x = torch.FloatTensor([[1, 2], [3, 4]])\r\n",
    "y = torch.FloatTensor([[5, 6], [7, 8]])\r\n",
    "\r\n",
    "# torch.cat 어느 차원을 늘릴 것인지 인자로 줄 수 있다.\r\n",
    "\r\n",
    "print(torch.cat([x, y], dim=0)) # 첫 번째 차원 늘리기\r\n",
    "print(torch.cat([x, y], dim=1)) # 두 번째 차원 늘리기"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.],\n",
      "        [5., 6.],\n",
      "        [7., 8.]])\n",
      "tensor([[1., 2., 5., 6.],\n",
      "        [3., 4., 7., 8.]])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "# Stacking : torch.stack([])\r\n",
    "\r\n",
    "x = torch.FloatTensor([1, 4])\r\n",
    "y = torch.FloatTensor([2, 5])\r\n",
    "z = torch.FloatTensor([3, 6])\r\n",
    "\r\n",
    "print(torch.stack([x, y, z]))\r\n",
    "\r\n",
    "print(torch.stack([x, y, z], dim=1))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[1., 4.],\n",
      "        [2., 5.],\n",
      "        [3., 6.]])\n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "# 1로 채워진 텐서 : torch.ones_like() \r\n",
    "# 동일한 크기(shape)지만 1으로만 값이 채워진 텐서를 생성\r\n",
    "x = torch.FloatTensor([[0, 1, 2], [2, 1, 0]])\r\n",
    "print(x)\r\n",
    "\r\n",
    "print(torch.ones_like(x))\r\n",
    "\r\n",
    "# 0으로 채워진 텐서 : torch.zeros_like() \r\n",
    "print(torch.zeros_like(x))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[0., 1., 2.],\n",
      "        [2., 1., 0.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "# 덮어쓰기 연산 : 연산 뒤에 _를 붙이면 기존의 값을 덮어쓰기 한다\r\n",
    "x = torch.FloatTensor([[1, 2], [3, 4]])\r\n",
    "\r\n",
    "print(x.mul(2.)) # 곱하기 2를 수행한 결과를 출력\r\n",
    "print(x) # 기존의 값 출력\r\n",
    "\r\n",
    "\r\n",
    "print(x.mul_(2.))  # 곱하기 2를 수행한 결과를 변수 x에 값을 저장하면서 결과를 출력\r\n",
    "print(x) # 기존의 값 출력"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[2., 4.],\n",
      "        [6., 8.]])\n",
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "tensor([[2., 4.],\n",
      "        [6., 8.]])\n",
      "tensor([[2., 4.],\n",
      "        [6., 8.]])\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('chch': conda)"
  },
  "interpreter": {
   "hash": "a64de8b745ab094eb3381810b1d090f0053b4977cc21c07d5a367fb401258f96"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}