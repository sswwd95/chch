{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "파이토치(PyTorch)에서는 텍스트에 대한 여러 추상화 기능을 제공하는 자연어 처리 라이브러리 토치텍스트(Torchtext)를 제공\r\n",
    "---\r\n",
    "* 파일 로드하기(File Loading) : 다양한 포맷의 코퍼스를 로드합니다.\r\n",
    "* 토큰화(Tokenization) : 문장을 단어 단위로 분리해줍니다.\r\n",
    "* 단어 집합(Vocab) : 단어 집합을 만듭니다.\r\n",
    "* 정수 인코딩(Integer encoding) : 전체 코퍼스의 단어들을 각각의 고유한 정수로 맵핑합니다.\r\n",
    "* 단어 벡터(Word Vector) : 단어 집합의 단어들에 고유한 임베딩 벡터를 만들어줍니다. 랜덤값으로 초기화한 값일 수도 있고, 사전 훈련된 임베딩 벡터들을 로드할 수도 있습니다.\r\n",
    "* 배치화(Batching) : 훈련 샘플들의 배치를 만들어줍니다. 이 과정에서 패딩 작업(Padding)도 이루어집니다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# !pip install torchtext"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "from torchtext.legacy.data import TabularDataset"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. 훈련 데이터와 테스트 데이터로 분리하기"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# 영화 사이트 IMDB의 리뷰 데이터\r\n",
    "import urllib.request\r\n",
    "import pandas as pd\r\n",
    "# urllib.request.urlretrieve(\"https://raw.githubusercontent.com/LawrenceDuan/IMDb-Review-Analysis/master/IMDb_Reviews.csv\", filename=\"IMDb_Reviews.csv\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "df = pd.read_csv('IMDb_Reviews.csv', encoding='latin1')\r\n",
    "df.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My family and I normally do not watch local mo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Believe it or not, this was at one time the wo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>After some internet surfing, I found the \"Home...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>One of the most unheralded great works of anim...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It was the Sixties, and anyone with long hair ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  My family and I normally do not watch local mo...          1\n",
       "1  Believe it or not, this was at one time the wo...          0\n",
       "2  After some internet surfing, I found the \"Home...          0\n",
       "3  One of the most unheralded great works of anim...          1\n",
       "4  It was the Sixties, and anyone with long hair ...          0"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "1은 긍정, 0은 부정"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "print('전체 샘플의 개수 : {}'.format(len(df)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "전체 샘플의 개수 : 50000\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "train_df = df[:25000]\r\n",
    "test_df = df[25000:]\r\n",
    "train_df.to_csv(\"train_data.csv\", index=False)\r\n",
    "test_df.to_csv(\"test_data.csv\", index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. 필드 정의하기(torchtext.data)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "* sequential : 시퀀스 데이터 여부. (True가 기본값)\r\n",
    "* use_vocab : 단어 집합을 만들 것인지 여부. (True가 기본값)\r\n",
    "* tokenize : 어떤 토큰화 함수를 사용할 것인지 지정. (string.split이 기본값)\r\n",
    "* lower : 영어 데이터를 전부 소문자화한다. (False가 기본값)\r\n",
    "* batch_first : 미니 배치 차원을 맨 앞으로 하여 데이터를 불러올 것인지 여부.(False가 기본값)\r\n",
    "* is_target : 레이블 데이터 여부. (False가 기본값)\r\n",
    "* fix_length : 최대 허용 길이. 이 길이에 맞춰서 패딩 작업(Padding)이 진행된다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "from torchtext.legacy import data # torchtext.data 임포트\r\n",
    "\r\n",
    "# 필드 정의\r\n",
    "\r\n",
    "# 실제 텍스트를 위한 text 객체\r\n",
    "TEXT = data.Field(sequential=True, #시퀀스 데이터\r\n",
    "                  use_vocab=True, # 단어 집합 만든다\r\n",
    "                  tokenize=str.split, # str으로 자르는 토큰화\r\n",
    "                  lower=True, # 전부 소문자\r\n",
    "                  batch_first=True, # 미니 배치 차원을 맨 앞으로해서 데이터 부름\r\n",
    "                  fix_length=20) # 최대 허용 길이, 이 길이에 맞춰 패딩작업 진행\r\n",
    "\r\n",
    "# 레이블 데이터를 위한 label 객체\r\n",
    "LABEL = data.Field(sequential=False,\r\n",
    "                   use_vocab=False,\r\n",
    "                   batch_first=False,\r\n",
    "                   is_target=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. 데이터셋 만들기\r\n",
    "TabularDataset은 데이터를 불러오면서 필드에서 정의했던 토큰화 방법으로 토큰화를 수행"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "from torchtext.legacy.data import TabularDataset\r\n",
    "\r\n",
    "train_data, test_data = TabularDataset.splits(\r\n",
    "        path='.', train='train_data.csv', test='test_data.csv', format='csv',\r\n",
    "        fields=[('text', TEXT), ('label', LABEL)], skip_header=True)\r\n",
    "\r\n",
    "# fields : 위에서 정의한 필드를 지정. 첫번째 원소는 데이터 셋 내에서 해당 필드를 호칭할 이름, 두번째 원소는 지정할 필드.\r\n",
    "# skip_header : 데이터의 첫번째 줄은 무시.\r\n",
    "\r\n",
    "print('훈련 샘플의 개수 : {}'.format(len(train_data)))\r\n",
    "print('테스트 샘플의 개수 : {}'.format(len(test_data)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "훈련 샘플의 개수 : 25000\n",
      "테스트 샘플의 개수 : 25000\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "print(vars(train_data[0])) # 주어진 인덱스 샘플 확인"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'text': ['my', 'family', 'and', 'i', 'normally', 'do', 'not', 'watch', 'local', 'movies', 'for', 'the', 'simple', 'reason', 'that', 'they', 'are', 'poorly', 'made,', 'they', 'lack', 'the', 'depth,', 'and', 'just', 'not', 'worth', 'our', 'time.<br', '/><br', '/>the', 'trailer', 'of', '\"nasaan', 'ka', 'man\"', 'caught', 'my', 'attention,', 'my', 'daughter', 'in', \"law's\", 'and', \"daughter's\", 'so', 'we', 'took', 'time', 'out', 'to', 'watch', 'it', 'this', 'afternoon.', 'the', 'movie', 'exceeded', 'our', 'expectations.', 'the', 'cinematography', 'was', 'very', 'good,', 'the', 'story', 'beautiful', 'and', 'the', 'acting', 'awesome.', 'jericho', 'rosales', 'was', 'really', 'very', 'good,', \"so's\", 'claudine', 'barretto.', 'the', 'fact', 'that', 'i', 'despised', 'diether', 'ocampo', 'proves', 'he', 'was', 'effective', 'at', 'his', 'role.', 'i', 'have', 'never', 'been', 'this', 'touched,', 'moved', 'and', 'affected', 'by', 'a', 'local', 'movie', 'before.', 'imagine', 'a', 'cynic', 'like', 'me', 'dabbing', 'my', 'eyes', 'at', 'the', 'end', 'of', 'the', 'movie?', 'congratulations', 'to', 'star', 'cinema!!', 'way', 'to', 'go,', 'jericho', 'and', 'claudine!!'], 'label': '1'}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "print(train_data[0]) "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<torchtext.legacy.data.example.Example object at 0x00000267AE8DFFA0>\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "print(train_data.fields.items())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "dict_items([('text', <torchtext.legacy.data.field.Field object at 0x00000267AEB8C310>), ('label', <torchtext.legacy.data.field.Field object at 0x00000267AEB8C280>)])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. 단어 집합(Vocabulary) 만들기\r\n",
    "\r\n",
    "토큰화 전처리를 끝냈다면, 이제 각 단어에 고유한 정수를 맵핑해주는 정수 인코딩(Integer enoding) 작업이 필요합니다. 그리고 이 전처리를 위해서는 우선 단어 집합을 만들어주어야 합니다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "TEXT.build_vocab(train_data, min_freq=10, max_size=10000)\r\n",
    "# min_freq : 단어 집합에 추가 시 단어의 최소 등장 빈도 조건을 추가.\r\n",
    "# max_size : 단어 집합의 최대 크기를 지정\r\n",
    "\r\n",
    "print('단어 집합의 크기 : {}'.format(len(TEXT.vocab)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "단어 집합의 크기 : 10002\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "# print(TEXT.vocab.stoi)\r\n",
    "print(type(TEXT.vocab.stoi))\r\n",
    "print(dict(list(TEXT.vocab.stoi.items())[:100]))\r\n",
    "# max size = 10002개인 이유 : unk, pad 들어갔기 때문!"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'collections.defaultdict'>\n",
      "{'<unk>': 0, '<pad>': 1, 'the': 2, 'a': 3, 'and': 4, 'of': 5, 'to': 6, 'is': 7, 'in': 8, 'i': 9, 'this': 10, 'it': 11, 'that': 12, '/><br': 13, 'was': 14, 'as': 15, 'for': 16, 'with': 17, 'but': 18, 'on': 19, 'movie': 20, 'his': 21, 'not': 22, 'are': 23, 'you': 24, 'film': 25, 'have': 26, 'he': 27, 'be': 28, 'at': 29, 'one': 30, 'by': 31, 'an': 32, 'they': 33, 'from': 34, 'who': 35, 'all': 36, 'like': 37, 'so': 38, 'just': 39, 'or': 40, 'has': 41, 'about': 42, \"it's\": 43, 'if': 44, 'her': 45, 'some': 46, 'out': 47, 'what': 48, 'very': 49, 'when': 50, 'more': 51, 'there': 52, 'even': 53, 'would': 54, 'my': 55, 'good': 56, 'she': 57, 'their': 58, 'only': 59, 'no': 60, 'really': 61, 'can': 62, 'up': 63, 'had': 64, 'which': 65, 'see': 66, 'were': 67, 'than': 68, '-': 69, 'we': 70, 'been': 71, 'into': 72, 'get': 73, 'much': 74, 'will': 75, 'because': 76, 'story': 77, 'most': 78, 'how': 79, 'other': 80, 'its': 81, 'first': 82, \"don't\": 83, 'time': 84, 'also': 85, 'do': 86, 'great': 87, 'me': 88, 'people': 89, 'make': 90, 'could': 91, 'any': 92, '/>the': 93, 'after': 94, 'made': 95, 'bad': 96, 'then': 97, 'think': 98, 'never': 99}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. 토치텍스트의 데이터로더 만들기"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "from torchtext.legacy.data import Iterator\r\n",
    "\r\n",
    "batch_size = 5\r\n",
    "\r\n",
    "train_loader = Iterator(dataset=train_data, batch_size = batch_size)\r\n",
    "test_loader = Iterator(dataset=test_data, batch_size = batch_size)\r\n",
    "print('훈련 데이터의 미니 배치 수 : {}'.format(len(train_loader)))\r\n",
    "print('테스트 데이터의 미니 배치 수 : {}'.format(len(test_loader)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "훈련 데이터의 미니 배치 수 : 5000\n",
      "테스트 데이터의 미니 배치 수 : 5000\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "batch = next(iter(train_loader)) # 첫번째 미니배치\r\n",
    "print(type(batch))\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'torchtext.legacy.data.batch.Batch'>\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "일반적인 데이터로더는 미니 배치를 텐서로 가져오지만 토치텍스트의 데이터로더는 torchtext.data.batch.Batch 객체를 가져온다.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "print(batch.text)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[  10,    7,   30,    5,  134,  127,  431,   16,  986, 2259,   39,    0,\n",
      "           17,  482,  287,   42,   11,    7, 1701, 2610],\n",
      "        [  44,   24,   67,  465,   31,  148,    0,    0,   69,   97,  407,   37,\n",
      "           10,   20,    0,    0,    2, 9855,    0,    7],\n",
      "        [1337,   98,    2,   82, 6094,   19,    2, 2926,   54,   28,  962,  221,\n",
      "          192, 8842,    6,   90,   63,  680,   42,  112],\n",
      "        [  11,   61,    0,   88,   12,   11,  178, 1055,    2,    0,  205,   28,\n",
      "           61, 4331,    8, 3870,    9,   62,   98,    5],\n",
      "        [2035, 7605,    7,    2,  499,  814, 1404,    0,    0,  110,    0,    0,\n",
      "            0, 1152,  872, 4504,   21,    0,   76,   60]])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6. <pad>토큰이 사용되는 경우\r\n",
    "\r\n",
    "맨 처음 필드를 정의할 때 fix_length를 20이 아니라 150으로 정의하면\r\n",
    "아래와 같은 결과로 나온다.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "'''\r\n",
    "tensor([ 248,   39,    0,    0,   55, 7701,    0,  174,  701,   34,    3,  403,\r\n",
    "           8,    0, 1480,    0, 2595, 1499,    0,    9,  388, 5068,    6,   73,\r\n",
    "           ... 중략 ...\r\n",
    "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\r\n",
    "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1])\r\n",
    "'''"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('chch': conda)"
  },
  "interpreter": {
   "hash": "a64de8b745ab094eb3381810b1d090f0053b4977cc21c07d5a367fb401258f96"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}