{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### 1. 임베딩 층(embedding layer)을 만들어 훈련 데이터로부터 처음부터 임베딩 벡터를 학습하는 방법\r\n",
    "### 2. 미리 사전에 훈련된 임베딩 벡터(pre-trained word embedding)들을 가져와 사용하는 방법"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. 임베딩 층은 룩업 테이블이다.\r\n",
    "임베딩 층의 입력으로 사용하기 위해서 입력 시퀀스의 각 단어들은 모두 정수 인코딩이 되어있어야 합니다.  \r\n",
    "\r\n",
    "어떤 단어 → 단어에 부여된 고유한 정수값 → 임베딩 층 통과 → 밀집 벡터  \r\n",
    "  \r\n",
    "  \r\n",
    "임베딩 층은 입력 정수에 대해 밀집 벡터(dense vector)로 맵핑하고 이 밀집 벡터는 인공 신경망의 학습 과정에서 가중치가 학습되는 것과 같은 방식으로 훈련됩니다. 훈련 과정에서 단어는 모델이 풀고자하는 작업에 맞는 값으로 업데이트 됩니다. 그리고 이 밀집 벡터를 임베딩 벡터라고 부릅니다.\r\n",
    "\r\n",
    "정수를 밀집 벡터 또는 임베딩 벡터로 맵핑한다는 것은 어떤 의미일까요? 특정 단어와 맵핑되는 정수를 인덱스로 가지는 테이블로부터 임베딩 벡터 값을 가져오는 룩업 테이블이라고 볼 수 있습니다. 그리고 이 테이블은 단어 집합의 크기만큼의 행을 가지므로 모든 단어는 고유한 임베딩 벡터를 가집니다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"https://wikidocs.net/images/page/33793/lookup_table.PNG\" width=\"400\" height=\"200\">"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "파이토치는 단어를 정수 인덱스로 바꾸고 원-핫 벡터로 한번 더 바꾸고나서 임베딩 층의 입력으로 사용하는 것이 아니라, 단어를 정수 인덱스로만 바꾼채로 임베딩 층의 입력으로 사용해도 룩업 테이블 된 결과인 임베딩 벡터를 리턴합니다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 룩업 테이블 과정을 nn.Embedding()을 사용하지 않고 구현"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "train_data = 'you need to know how to code'\r\n",
    "word_set = set(train_data.split()) # 중복을 제거한 단어들의 집합인 단어 집합 생성.\r\n",
    "print(word_set)\r\n",
    "# unk, pad가 0,1이니까 i는 +2부터\r\n",
    "vocab = {word: i+2 for i, word in enumerate(word_set)}  # 단어 집합의 각 단어에 고유한 정수 맵핑.\r\n",
    "vocab['<unk>'] = 0\r\n",
    "vocab['<pad>'] = 1\r\n",
    "print(vocab)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'you', 'know', 'need', 'how', 'to', 'code'}\n",
      "{'you': 2, 'know': 3, 'need': 4, 'how': 5, 'to': 6, 'code': 7, '<unk>': 0, '<pad>': 1}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "import torch\r\n",
    "# 단어 집합의 크기만큼의 행을 가지는 테이블 생성.\r\n",
    "embedding_table = torch.FloatTensor([\r\n",
    "                               [ 0.0,  0.0,  0.0],\r\n",
    "                               [ 0.0,  0.0,  0.0],\r\n",
    "                               [ 0.2,  0.9,  0.3],\r\n",
    "                               [ 0.1,  0.5,  0.7],\r\n",
    "                               [ 0.2,  0.1,  0.8],\r\n",
    "                               [ 0.4,  0.1,  0.1],\r\n",
    "                               [ 0.1,  0.8,  0.9],\r\n",
    "                               [ 0.6,  0.1,  0.1]])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# 임의의 샘플 문장\r\n",
    "sample = 'you need to run'.split()\r\n",
    "idxes=[]\r\n",
    "# 각 단어를 정수로 변환\r\n",
    "for word in sample:\r\n",
    "  try:\r\n",
    "    idxes.append(vocab[word])\r\n",
    "  except KeyError: # 단어 집합에 없는 단어일 경우 <unk>로 대체된다.\r\n",
    "    idxes.append(vocab['<unk>'])\r\n",
    "idxes = torch.LongTensor(idxes)\r\n",
    "\r\n",
    "# 룩업 테이블\r\n",
    "lookup_result = embedding_table[idxes, :] # 각 정수를 인덱스로 임베딩 테이블에서 값을 가져온다.\r\n",
    "print(lookup_result)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[0.2000, 0.9000, 0.3000],\n",
      "        [0.2000, 0.1000, 0.8000],\n",
      "        [0.1000, 0.8000, 0.9000],\n",
      "        [0.0000, 0.0000, 0.0000]])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "train_data = 'you need to know how to code'\r\n",
    "word_set = set(train_data.split()) # 중복을 제거한 단어들의 집합인 단어 집합 생성.\r\n",
    "vocab = {tkn: i+2 for i, tkn in enumerate(word_set)}  # 단어 집합의 각 단어에 고유한 정수 맵핑.\r\n",
    "vocab['<unk>'] = 0\r\n",
    "vocab['<pad>'] = 1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "import torch.nn as nn\r\n",
    "embedding_layer = nn.Embedding(num_embeddings = len(vocab), \r\n",
    "                               embedding_dim = 3,\r\n",
    "                               padding_idx = 1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "* num_embeddings : 임베딩을 할 단어들의 개수. 다시 말해 단어 집합의 크기입니다.\r\n",
    "* embedding_dim : 임베딩 할 벡터의 차원입니다. 사용자가 정해주는 하이퍼파라미터입니다.\r\n",
    "* padding_idx : 선택적으로 사용하는 인자입니다. 패딩을 위한 토큰의 인덱스를 알려줍니다"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "print(embedding_layer.weight)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.1343, -0.0078,  0.0827],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [-0.3114,  0.6054,  0.0993],\n",
      "        [ 0.2750,  1.1515, -0.6616],\n",
      "        [-0.7144,  0.3600,  0.5290],\n",
      "        [-0.9022,  0.0855, -0.3383],\n",
      "        [-0.1096, -0.1819,  1.1943],\n",
      "        [ 0.4301,  0.2479,  1.8357]], requires_grad=True)\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('chch': conda)"
  },
  "interpreter": {
   "hash": "a64de8b745ab094eb3381810b1d090f0053b4977cc21c07d5a367fb401258f96"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}