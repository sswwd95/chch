{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 04. 영어/한국어 Word2Vec 훈련시키기"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "파이썬의 gensim 패키지에는 Word2Vec을 지원하고 있어, gensim 패키지를 이용하면 손쉽게 단어를 임베딩 벡터로 변환시킬 수 있습니다. 영어로 된 코퍼스를 다운받아 전처리를 수행하고, 전처리한 데이터를 바탕으로 Word2Vec 작업을 진행하겠습니다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import nltk\r\n",
    "nltk.download('punkt')\r\n",
    "import urllib.request\r\n",
    "import zipfile\r\n",
    "from lxml import etree\r\n",
    "import re\r\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sswwd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/GaoleMeng/RNN-and-FFNN-textClassification/master/ted_en-20160408.xml\", filename=\"ted_en-20160408.xml\")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('ted_en-20160408.xml', <http.client.HTTPMessage at 0x172e67236a0>)"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "해당 파일은 xml 문법으로 작성되어 있어 자연어를 얻기 위해서는 전처리가 필요하다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "targetXML=open('ted_en-20160408.xml', 'r', encoding='UTF8')\r\n",
    "# 저자의 경우 윈도우 바탕화면에서 작업하여서 'C:\\Users\\USER\\Desktop\\ted_en-20160408.xml'이 해당 파일의 경로.  \r\n",
    "target_text = etree.parse(targetXML)\r\n",
    "parse_text = '\\n'.join(target_text.xpath('//content/text()'))\r\n",
    "# xml 파일로부터 <content>와 </content> 사이의 내용만 가져온다.\r\n",
    "\r\n",
    "content_text = re.sub(r'\\([^)]*\\)', '', parse_text)\r\n",
    "# 정규 표현식의 sub 모듈을 통해 content 중간에 등장하는 (Audio), (Laughter) 등의 배경음 부분을 제거.\r\n",
    "# 해당 코드는 괄호로 구성된 내용을 제거.\r\n",
    "\r\n",
    "sent_text = sent_tokenize(content_text)\r\n",
    "# 입력 코퍼스에 대해서 NLTK를 이용하여 문장 토큰화를 수행.\r\n",
    "\r\n",
    "normalized_text = []\r\n",
    "for string in sent_text:\r\n",
    "     tokens = re.sub(r\"[^a-z0-9]+\", \" \", string.lower())\r\n",
    "     normalized_text.append(tokens)\r\n",
    "# 각 문장에 대해서 구두점을 제거하고, 대문자를 소문자로 변환.\r\n",
    "\r\n",
    "result = []\r\n",
    "result = [word_tokenize(sentence) for sentence in normalized_text]\r\n",
    "\r\n",
    "print('총 샘플의 개수 : {}'.format(len(result)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "총 샘플의 개수 : 273424\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "for line in result[:3]: # 샘플 3개만 출력\r\n",
    "    print(line)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['here', 'are', 'two', 'reasons', 'companies', 'fail', 'they', 'only', 'do', 'more', 'of', 'the', 'same', 'or', 'they', 'only', 'do', 'what', 's', 'new']\n",
      "['to', 'me', 'the', 'real', 'real', 'solution', 'to', 'quality', 'growth', 'is', 'figuring', 'out', 'the', 'balance', 'between', 'two', 'activities', 'exploration', 'and', 'exploitation']\n",
      "['both', 'are', 'necessary', 'but', 'it', 'can', 'be', 'too', 'much', 'of', 'a', 'good', 'thing']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3) Word2Vec 훈련시키기\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# !pip install gensim"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors\r\n",
    "model = Word2Vec(sentences=result, vector_size=100, window=5, min_count=5, workers=4, sg=0)\r\n",
    "# cbow 사용\r\n",
    "from gensim.models import Word2Vec, KeyedVectors\r\n",
    "model2 = Word2Vec(sentences=result, vector_size=100, window=5, min_count=5, workers=4, sg=1)\r\n",
    "# skip-gram 사용"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "여기서 Word2Vec의 하이퍼파라미터값은 다음과 같습니다.  \r\n",
    "size = 워드 벡터의 특징 값. 즉, 임베딩 된 벡터의 차원.  \r\n",
    "window = 컨텍스트 윈도우 크기  \r\n",
    "min_count = 단어 최소 빈도 수 제한 (빈도가 적은 단어들은 학습하지 않는다.)  \r\n",
    "workers = 학습을 위한 프로세스 수  \r\n",
    "sg = 0은 CBOW, 1은 Skip-gram.  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "model_result = model.wv.most_similar(\"man\") #가장 유사한 단어 출력\r\n",
    "print(model_result)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('woman', 0.8452662825584412), ('guy', 0.7914076447486877), ('lady', 0.7580448389053345), ('boy', 0.7566693425178528), ('girl', 0.7327544093132019), ('gentleman', 0.7280296087265015), ('soldier', 0.723828911781311), ('poet', 0.6915566921234131), ('kid', 0.6702145338058472), ('surgeon', 0.6474255919456482)]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "model_result2 = model2.wv.most_similar(\"man\") #가장 유사한 단어 출력\r\n",
    "print(model_result2)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('woman', 0.7728009819984436), ('guy', 0.7634276747703552), ('boy', 0.7009593844413757), ('rabbi', 0.6902947425842285), ('titus', 0.6895440816879272), ('soldier', 0.6821202635765076), ('son', 0.6767211556434631), ('pianist', 0.6747886538505554), ('gentleman', 0.6726687550544739), ('widow', 0.6685516834259033)]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4) Word2Vec 모델 저장하고 로드하기"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "model.wv.save_word2vec_format('./eng_w2v') # 모델 저장\r\n",
    "loaded_model = KeyedVectors.load_word2vec_format(\"eng_w2v\") # 모델 로드"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "model_result = loaded_model.most_similar(\"man\")\r\n",
    "print(model_result)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('woman', 0.8452662825584412), ('guy', 0.7914076447486877), ('lady', 0.7580448389053345), ('boy', 0.7566693425178528), ('girl', 0.7327544093132019), ('gentleman', 0.7280296087265015), ('soldier', 0.723828911781311), ('poet', 0.6915566921234131), ('kid', 0.6702145338058472), ('surgeon', 0.6474255919456482)]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 한국어 word2vec 만들기"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. https://dumps.wikimedia.org/kowiki/latest/ 에 들어가서   \r\n",
    "kowiki-latest-pages-articles.xml.bz2 파일 다운로드(약 40분걸림)    \r\n",
    "2. https://github.com/attardi/wikiextractor 들어가서 code를 zip으로 다운로드   \r\n",
    "\r\n",
    "\r\n",
    "3. 압축 풀고 cmd에서 파일 있는 경로로 가서 가상환경 설정해주고 'python setdup.py install'입력  \r\n",
    "  \r\n",
    "  \r\n",
    "4. wikiextractor 폴더 안에 WikiExtractor.py 파일이 생기는데   \r\n",
    "동일한 경로 안에 kowiki-latest-pages-articles.xml.bz2 넣어준다.  \r\n",
    "    * A:\\chchdata\\data\\wikiextractor-master\\wikiextractor\\WikiExtractor.py  \r\n",
    "    * A:\\chchdata\\data\\wikiextractor-master\\wikiextractor\\kowiki-latest-pages-articles.xml.bz2\r\n",
    "\r\n",
    "5. WikiExtractor.py에 들어가서 파일 변경(YoungriKIM의 코드 참고)\r\n",
    "    - from multiprocessing.dummy import Queue, Process 추가\r\n",
    "    - 186번 줄 : return open(filename, 'w') -> return open(filename, 'w', encoding='utf-8')\r\n",
    "    - 210번 줄 : output = open(output_file, 'w') -> output = open(output_file, 'w', encoding='utf-8')\r\n",
    "\r\n",
    "5. cmd에 위의 경로로 들어가서 python -m wikiextractor.WikiExtractor kowiki-latest-pages-articles.xml.bz2 입력\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 훈련데이터 만들기\r\n",
    "### AA 디렉토리 안의 모든 파일인 wiki00 ~ wiki90에 대해서 wikiAA.txt로 통합\r\n",
    "cmd 창에서 아래와 같이 입력  \r\n",
    "(chch) A:\\chch\\chch\\wikiextractor\\text>  \r\n",
    "copy A:\\chch\\chch\\wikiextractor\\text\\AA\\wiki* wiki1.txt   \r\n",
    "copy A:\\chch\\chch\\wikiextractor\\text\\AB\\wiki* wiki2.txt  \r\n",
    "copy A:\\chch\\chch\\wikiextractor\\text\\AC\\wiki* wiki3.txt  \r\n",
    "copy A:\\chch\\chch\\wikiextractor\\text\\AD\\wiki* wiki4.txt  \r\n",
    "copy A:\\chch\\chch\\wikiextractor\\text\\AE\\wiki* wiki5.txt  \r\n",
    "copy A:\\chch\\chch\\wikiextractor\\text\\AF\\wiki* wiki6.txt  \r\n",
    "copy A:\\chch\\chch\\wikiextractor\\text\\AG\\wiki* wiki7.txt  \r\n",
    "copy A:\\chch\\chch\\wikiextractor\\text\\AH\\wiki* wiki8.txt  \r\n",
    "copy A:\\chch\\chch\\wikiextractor\\text\\AI\\wiki* wiki9.txt  \r\n",
    "\r\n",
    "### 위의 파일을 다시 1개의 파일로 합치기\r\n",
    "copy A:\\chch\\chch\\wikiextractor\\text\\wiki* wiki_data.txt\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "f = open('A:\\\\chch\\\\chch\\\\wikiextractor\\\\text\\\\wiki_data.txt', encoding=\"utf8\")\r\n",
    "\r\n",
    "i=0\r\n",
    "while True:\r\n",
    "    line = f.readline()\r\n",
    "    if line != '\\n':\r\n",
    "        i=i+1\r\n",
    "        print(\"%d번째 줄 :\"%i + line)\r\n",
    "    if i==5:\r\n",
    "        break \r\n",
    "f.close()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1번째 줄 :<doc id=\"5\" url=\"https://ko.wikipedia.org/wiki?curid=5\" title=\"지미 카터\">\n",
      "\n",
      "2번째 줄 :지미 카터\n",
      "\n",
      "3번째 줄 :제임스 얼 카터 주니어(, 1924년 10월 1일 ~ )는 민주당 출신 미국 39대 대통령 (1977년 ~ 1981년)이다.\n",
      "\n",
      "4번째 줄 :생애.\n",
      "\n",
      "5번째 줄 :어린 시절.\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "from konlpy.tag import Okt\r\n",
    "from datetime import datetime\r\n",
    "\r\n",
    "start = datetime.now()\r\n",
    "okt=Okt()\r\n",
    "fread = open('A:\\\\chch\\\\chch\\\\wikiextractor\\\\text\\\\wiki_data.txt', encoding=\"utf8\")\r\n",
    "# 파일을 다시 처음부터 읽음.\r\n",
    "n=0\r\n",
    "result = []\r\n",
    "\r\n",
    "while True:\r\n",
    "    line = fread.readline() #한 줄씩 읽음.\r\n",
    "    if not line: break # 모두 읽으면 while문 종료.\r\n",
    "    n=n+1\r\n",
    "    if n%5000==0: # 5,000의 배수로 While문이 실행될 때마다 몇 번째 While문 실행인지 출력.\r\n",
    "        print(\"%d번째 While문.\"%n)\r\n",
    "    tokenlist = okt.pos(line, stem=True, norm=True) # 단어 토큰화\r\n",
    "    temp=[]\r\n",
    "    for word in tokenlist:\r\n",
    "        if word[1] in [\"Noun\"]: # 명사일 때만\r\n",
    "            temp.append((word[0])) # 해당 단어를 저장함\r\n",
    "\r\n",
    "    if temp: # 만약 이번에 읽은 데이터에 명사가 존재할 경우에만\r\n",
    "      result.append(temp) # 결과에 저장\r\n",
    "fread.close()\r\n",
    "end = datetime.now()\r\n",
    "print('시간 : ',end-start)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "5000번째 While문.\n",
      "10000번째 While문.\n",
      "15000번째 While문.\n",
      "20000번째 While문.\n",
      "25000번째 While문.\n",
      "30000번째 While문.\n",
      "35000번째 While문.\n",
      "40000번째 While문.\n",
      "45000번째 While문.\n",
      "50000번째 While문.\n",
      "55000번째 While문.\n",
      "60000번째 While문.\n",
      "65000번째 While문.\n",
      "70000번째 While문.\n",
      "75000번째 While문.\n",
      "80000번째 While문.\n",
      "85000번째 While문.\n",
      "90000번째 While문.\n",
      "95000번째 While문.\n",
      "100000번째 While문.\n",
      "105000번째 While문.\n",
      "110000번째 While문.\n",
      "115000번째 While문.\n",
      "120000번째 While문.\n",
      "125000번째 While문.\n",
      "130000번째 While문.\n",
      "135000번째 While문.\n",
      "140000번째 While문.\n",
      "145000번째 While문.\n",
      "150000번째 While문.\n",
      "155000번째 While문.\n",
      "160000번째 While문.\n",
      "165000번째 While문.\n",
      "170000번째 While문.\n",
      "175000번째 While문.\n",
      "180000번째 While문.\n",
      "185000번째 While문.\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-04a2eb582636>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m5000\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# 5,000의 배수로 While문이 실행될 때마다 몇 번째 While문 실행인지 출력.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%d번째 While문.\"\u001b[0m\u001b[1;33m%\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0mtokenlist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mokt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstem\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 단어 토큰화\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[0mtemp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokenlist\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\chch\\lib\\site-packages\\konlpy\\tag\\_okt.py\u001b[0m in \u001b[0;36mpos\u001b[1;34m(self, phrase, norm, stem, join)\u001b[0m\n\u001b[0;32m     58\u001b[0m         \"\"\"\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m         tokens = self.jki.tokenize(\n\u001b[0m\u001b[0;32m     61\u001b[0m                     \u001b[0mphrase\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m                     \u001b[0mjpype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBoolean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print('총 샘플의 개수 : {}'.format(len(result)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "총 샘플의 개수 : 1657878\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Word2Vec 훈련시키기"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from gensim.models import Word2Vec\r\n",
    "model = Word2Vec(result, vector_size=100, window=5, min_count=5, workers=4, sg=0)\r\n",
    "model2 = Word2Vec(result, vector_size=100, window=5, min_count=5, workers=4, sg=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model_result1=model.wv.most_similar(\"대한민국\")\r\n",
    "print(model_result1)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('한국', 0.6591412425041199), ('조선민주주의인민공화국', 0.5870327353477478), ('관세청', 0.5554245710372925), ('우리나라', 0.5490413904190063), ('부산광역시', 0.5448193550109863), ('국내', 0.5182844996452332), ('대구광역시', 0.5025010108947754), ('서울특별시', 0.50123131275177), ('교육인', 0.48795831203460693), ('경상북도', 0.48792165517807007)]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model_result2=model2.wv.most_similar(\"대한민국\")\r\n",
    "print(model_result2)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('한국', 0.6806833744049072), ('김정훈', 0.6688888669013977), ('정미경', 0.6647691130638123), ('탁구선수', 0.6479456424713135), ('최은경', 0.6457558870315552), ('이현진', 0.6324021220207214), ('이현욱', 0.6303592920303345), ('박신영', 0.6282212734222412), ('장복심', 0.6275299191474915), ('장용석', 0.6274713277816772)]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model_result3=model.wv.most_similar(\"고양이\")\r\n",
    "print(model_result3)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('강아지', 0.7985575795173645), ('거위', 0.7776944637298584), ('토끼', 0.7718325257301331), ('애완동물', 0.7614423632621765), ('애완견', 0.7593047618865967), ('울음소리', 0.7482358813285828), ('거북이', 0.7399520874023438), ('개구리', 0.7254258990287781), ('사냥개', 0.7131367921829224), ('생쥐', 0.6942580342292786)]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model_result4=model2.wv.most_similar(\"고양이\")\r\n",
    "print(model_result4)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('거위', 0.7450876235961914), ('애완견', 0.7371352314949036), ('카멜레온', 0.7318968772888184), ('생쥐', 0.7276090383529663), ('들쥐', 0.7273074984550476), ('햄스터', 0.7257925868034363), ('토끼', 0.7210389375686646), ('더치스', 0.7160189747810364), ('앵무새', 0.7088590264320374), ('바케네코', 0.7079804539680481)]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 사전 훈련된 워드 임베딩 사용"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. 영어 \r\n",
    "구글은 사전 훈련된 3백만 개의 Word2Vec 단어 벡터들을 제공합니다. 각 임베딩 벡터의 차원은 300입니다. gensim을 통해서 이 모델을 불러오는 건 매우 간단합니다. 이 모델을 다운로드하고 파일 경로를 기재하면 됩니다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import gensim\r\n",
    "\r\n",
    "# 구글의 사전 훈련된 Word2Vec 모델을 로드합니다.\r\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('A:\\\\chchdata\\\\data\\\\GoogleNews-vectors-negative300.bin\\\\GoogleNews-vectors-negative300.bin', binary=True) \r\n",
    "print(model.vectors.shape) # 모델의 크기 확인"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(3000000, 300)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Load a word2vec model stored in the C *text* format.  \r\n",
    "wv_from_text = KeyedVectors.load_word2vec_format(datapath('word2vec_pre_kv_c'), binary=False)\r\n",
    "#### Load a word2vec model stored in the C *binary* format.  \r\n",
    "wv_from_bin = KeyedVectors.load_word2vec_format(datapath(\"euclidean_vectors.bin\"), binary=True)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print (model.similarity('this', 'is')) # 두 단어의 유사도 계산하기\r\n",
    "print (model.similarity('post', 'book'))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.40797037\n",
      "0.057204388\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(model['book']) # 단어 'book'의 벡터 출력"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[ 0.11279297 -0.02612305 -0.04492188  0.06982422  0.140625    0.03039551\n",
      " -0.04370117  0.24511719  0.08740234 -0.05053711  0.23144531 -0.07470703\n",
      "  0.21875     0.03466797 -0.14550781  0.05761719  0.00671387 -0.00701904\n",
      "  0.13183594 -0.25390625  0.14355469 -0.140625   -0.03564453 -0.21289062\n",
      " -0.24804688  0.04980469 -0.09082031  0.14453125  0.05712891 -0.10400391\n",
      " -0.19628906 -0.20507812 -0.27539062  0.03063965  0.20117188  0.17382812\n",
      "  0.09130859 -0.10107422  0.22851562 -0.04077148  0.02709961 -0.00106049\n",
      "  0.02709961  0.34179688 -0.13183594 -0.078125    0.02197266 -0.18847656\n",
      " -0.17480469 -0.05566406 -0.20898438  0.04858398 -0.07617188 -0.15625\n",
      " -0.05419922  0.01672363 -0.02722168 -0.11132812 -0.03588867 -0.18359375\n",
      "  0.28710938  0.01757812  0.02185059 -0.05664062 -0.01251221  0.01708984\n",
      " -0.21777344 -0.06787109  0.04711914 -0.00668335  0.08544922 -0.02209473\n",
      "  0.31835938  0.01794434 -0.02246094 -0.03051758 -0.09570312  0.24414062\n",
      "  0.20507812  0.05419922  0.29101562  0.03637695  0.04956055 -0.06689453\n",
      "  0.09277344 -0.10595703 -0.04370117  0.19726562 -0.03015137  0.05615234\n",
      "  0.08544922 -0.09863281 -0.02392578 -0.08691406 -0.22460938 -0.16894531\n",
      "  0.09521484 -0.0612793  -0.03015137 -0.265625   -0.13378906  0.00139618\n",
      "  0.01794434  0.10107422  0.13964844  0.06445312 -0.09765625 -0.11376953\n",
      " -0.24511719 -0.15722656  0.00457764  0.12988281 -0.03540039 -0.08105469\n",
      "  0.18652344  0.03125    -0.09326172 -0.04760742  0.23730469  0.11083984\n",
      "  0.08691406  0.01916504  0.21386719 -0.0065918  -0.08984375 -0.02502441\n",
      " -0.09863281 -0.05639648 -0.26757812  0.19335938 -0.08886719 -0.25976562\n",
      "  0.05957031 -0.10742188  0.09863281  0.1484375   0.04101562  0.00340271\n",
      " -0.06591797 -0.02941895  0.20019531 -0.00521851  0.02355957 -0.13671875\n",
      " -0.12597656 -0.10791016  0.0067749   0.15917969  0.0145874  -0.15136719\n",
      "  0.07519531 -0.02905273  0.01843262  0.20800781  0.25195312 -0.11523438\n",
      " -0.23535156  0.04101562 -0.11035156  0.02905273  0.22460938 -0.04272461\n",
      "  0.09667969  0.11865234  0.08007812  0.07958984  0.3125     -0.14941406\n",
      " -0.234375    0.06079102  0.06982422 -0.14355469 -0.05834961 -0.36914062\n",
      " -0.10595703  0.00738525  0.24023438 -0.10400391 -0.02124023  0.05712891\n",
      " -0.11621094 -0.16894531 -0.06396484 -0.12060547  0.08105469 -0.13769531\n",
      " -0.08447266  0.12792969 -0.15429688  0.17871094  0.2421875  -0.06884766\n",
      "  0.03320312  0.04394531 -0.04589844  0.03686523 -0.07421875 -0.01635742\n",
      " -0.24121094 -0.08203125 -0.01733398  0.0291748   0.10742188  0.11279297\n",
      "  0.12890625  0.01416016 -0.28710938  0.16503906 -0.25585938  0.2109375\n",
      " -0.19238281  0.22363281  0.04541016  0.00872803  0.11376953  0.375\n",
      "  0.09765625  0.06201172  0.12109375 -0.24316406  0.203125    0.12158203\n",
      "  0.08642578  0.01782227  0.17382812  0.01855469  0.03613281 -0.02124023\n",
      " -0.02905273 -0.04541016  0.1796875   0.06494141 -0.13378906 -0.09228516\n",
      "  0.02172852  0.02099609  0.07226562  0.3046875  -0.27539062 -0.30078125\n",
      "  0.08691406 -0.22949219  0.0546875  -0.34179688 -0.00680542 -0.0291748\n",
      " -0.03222656  0.16210938  0.01141357  0.23339844 -0.0859375  -0.06494141\n",
      "  0.15039062  0.17675781  0.08251953 -0.26757812 -0.11669922  0.01330566\n",
      "  0.01818848  0.10009766 -0.09570312  0.109375   -0.16992188 -0.23046875\n",
      " -0.22070312  0.0625      0.03662109 -0.125       0.05151367 -0.18847656\n",
      "  0.22949219  0.26367188 -0.09814453  0.06176758  0.11669922  0.23046875\n",
      "  0.32617188  0.02038574 -0.03735352 -0.12255859  0.296875   -0.25\n",
      " -0.08544922 -0.03149414  0.38085938  0.02929688 -0.265625    0.42382812\n",
      " -0.1484375   0.14355469 -0.03125     0.00717163 -0.16601562 -0.15820312\n",
      "  0.03637695 -0.16796875 -0.01483154  0.09667969 -0.05761719 -0.00515747]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. 한국어"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!pip uninstall -y gensim"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found existing installation: gensim 4.1.2\n",
      "Uninstalling gensim-4.1.2:\n",
      "  Successfully uninstalled gensim-4.1.2\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!pip install --upgrade --user gensim==3.8.3"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting gensim==3.8.3\n",
      "  Using cached gensim-3.8.3-cp38-cp38-win_amd64.whl (24.2 MB)\n",
      "Requirement already satisfied: six>=1.5.0 in c:\\users\\sswwd\\anaconda3\\envs\\chch\\lib\\site-packages (from gensim==3.8.3) (1.16.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\sswwd\\anaconda3\\envs\\chch\\lib\\site-packages (from gensim==3.8.3) (1.7.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\sswwd\\anaconda3\\envs\\chch\\lib\\site-packages (from gensim==3.8.3) (5.2.1)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\users\\sswwd\\anaconda3\\envs\\chch\\lib\\site-packages (from gensim==3.8.3) (1.21.1)\n",
      "Collecting Cython==0.29.14\n",
      "  Using cached Cython-0.29.14-cp38-cp38-win_amd64.whl (1.7 MB)\n",
      "Installing collected packages: Cython, gensim\n",
      "  Attempting uninstall: Cython\n",
      "    Found existing installation: Cython 0.29.23\n",
      "    Uninstalling Cython-0.29.23:\n",
      "      Successfully uninstalled Cython-0.29.23\n",
      "  Attempting uninstall: gensim\n",
      "    Found existing installation: gensim 3.8.1\n",
      "    Uninstalling gensim-3.8.1:\n",
      "      Successfully uninstalled gensim-3.8.1\n",
      "Successfully installed Cython-0.29.14 gensim-3.8.3\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  WARNING: The scripts cygdb.exe, cython.exe and cythonize.exe are installed in 'C:\\Users\\sswwd\\AppData\\Roaming\\Python\\Python38\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!pip list"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Package               Version\n",
      "--------------------- -------------------\n",
      "argon2-cffi           20.1.0\n",
      "async-generator       1.10\n",
      "attrs                 21.2.0\n",
      "backcall              0.2.0\n",
      "beautifulsoup4        4.6.0\n",
      "bleach                4.0.0\n",
      "blis                  0.7.4\n",
      "catalogue             2.0.6\n",
      "certifi               2021.5.30\n",
      "cffi                  1.14.6\n",
      "charset-normalizer    2.0.6\n",
      "click                 7.1.2\n",
      "colorama              0.4.4\n",
      "cycler                0.10.0\n",
      "cymem                 2.0.5\n",
      "Cython                0.29.14\n",
      "decorator             5.0.9\n",
      "defusedxml            0.7.1\n",
      "en-core-web-sm        3.1.0\n",
      "entrypoints           0.3\n",
      "gensim                3.8.3\n",
      "idna                  3.2\n",
      "ipykernel             5.3.4\n",
      "ipython               7.22.0\n",
      "ipython-genutils      0.2.0\n",
      "ipywidgets            7.6.3\n",
      "jedi                  0.17.0\n",
      "Jinja2                3.0.1\n",
      "joblib                1.0.1\n",
      "JPype1                1.1.2\n",
      "jsonschema            3.2.0\n",
      "jupyter               1.0.0\n",
      "jupyter-client        6.1.12\n",
      "jupyter-console       6.4.0\n",
      "jupyter-core          4.7.1\n",
      "jupyterlab-pygments   0.1.2\n",
      "jupyterlab-widgets    1.0.0\n",
      "kiwisolver            1.3.1\n",
      "konlpy                0.5.2\n",
      "lxml                  4.6.3\n",
      "MarkupSafe            2.0.1\n",
      "matplotlib            3.4.2\n",
      "mistune               0.8.4\n",
      "murmurhash            1.0.5\n",
      "nbclient              0.5.3\n",
      "nbconvert             6.1.0\n",
      "nbformat              5.1.3\n",
      "nest-asyncio          1.5.1\n",
      "nltk                  3.6.2\n",
      "notebook              6.4.3\n",
      "numpy                 1.21.1\n",
      "oauthlib              3.1.1\n",
      "olefile               0.46\n",
      "packaging             21.0\n",
      "pandas                1.3.0\n",
      "pandocfilters         1.4.3\n",
      "parso                 0.8.2\n",
      "pathy                 0.6.0\n",
      "pickleshare           0.7.5\n",
      "Pillow                8.3.1\n",
      "pip                   21.2.4\n",
      "preshed               3.0.5\n",
      "prometheus-client     0.11.0\n",
      "prompt-toolkit        3.0.17\n",
      "pycparser             2.20\n",
      "pydantic              1.8.2\n",
      "Pygments              2.9.0\n",
      "pyparsing             2.4.7\n",
      "pyrsistent            0.18.0\n",
      "PySocks               1.7.1\n",
      "python-dateutil       2.8.2\n",
      "pytorch-model-summary 0.1.2\n",
      "pytz                  2021.1\n",
      "pywin32               227\n",
      "pywinpty              1.1.3\n",
      "pyzmq                 20.0.0\n",
      "qtconsole             5.1.1\n",
      "QtPy                  1.9.0\n",
      "regex                 2021.8.28\n",
      "requests              2.26.0\n",
      "requests-oauthlib     1.3.0\n",
      "scikit-learn          0.24.2\n",
      "scipy                 1.7.1\n",
      "Send2Trash            1.8.0\n",
      "setuptools            52.0.0.post20210125\n",
      "six                   1.16.0\n",
      "sklearn               0.0\n",
      "smart-open            5.2.1\n",
      "spacy                 3.1.2\n",
      "spacy-legacy          3.0.8\n",
      "srsly                 2.4.1\n",
      "terminado             0.10.1\n",
      "testpath              0.5.0\n",
      "thinc                 8.0.10\n",
      "threadpoolctl         2.2.0\n",
      "torch                 1.9.0\n",
      "torchaudio            0.9.0\n",
      "torchsummary          1.5.1\n",
      "torchvision           0.10.0\n",
      "tornado               6.1\n",
      "tqdm                  4.62.1\n",
      "traitlets             5.0.5\n",
      "transforms            0.1\n",
      "tweepy                3.7.0\n",
      "typer                 0.3.2\n",
      "typing-extensions     3.10.0.0\n",
      "urllib3               1.26.6\n",
      "wasabi                0.8.2\n",
      "wcwidth               0.2.5\n",
      "webencodings          0.5.1\n",
      "wheel                 0.36.2\n",
      "widgetsnbextension    3.5.1\n",
      "wikiextractor         3.0.5\n",
      "wincertstore          0.2\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import gensim\r\n",
    "from gensim.models import KeyedVectors\r\n",
    "model = gensim.models.Word2Vec.load('A:\\\\chchdata\\\\data\\\\ko\\\\ko.bin')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "result=model.wv.most_similar(\"강아지\")\r\n",
    "print(result)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('고양이', 0.7290453314781189), ('거위', 0.7185634970664978), ('토끼', 0.7056223750114441), ('멧돼지', 0.6950401067733765), ('엄마', 0.693433403968811), ('난쟁이', 0.6806551218032837), ('한마리', 0.6770296096801758), ('아가씨', 0.675035297870636), ('아빠', 0.6729634404182434), ('목걸이', 0.6512461304664612)]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "result=model.wv.most_similar(\"인공지능\")\r\n",
    "print(result)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('컴퓨팅', 0.6520194411277771), ('가상현실', 0.6393702030181885), ('심리학', 0.63037109375), ('모델링', 0.625065267086029), ('신경망', 0.6200423836708069), ('로봇', 0.6109743118286133), ('시뮬레이션', 0.6101070642471313), ('지능', 0.6092982888221741), ('기술', 0.6087721586227417), ('기술인', 0.5957076549530029)]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.wv.save_word2vec_format('./ko_w2v') # 모델 저장\r\n",
    "# loaded_model = KeyedVectors.load_word2vec_format(\"eng_w2v\") # 모델 로드"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('chch': conda)"
  },
  "interpreter": {
   "hash": "a64de8b745ab094eb3381810b1d090f0053b4977cc21c07d5a367fb401258f96"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}