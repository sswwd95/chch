{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 1.희소 표현(Sparse Representation) = 원-핫 벡터\r\n",
    "벡터 또는 행렬(matrix)의 값이 대부분이 0으로 표현되는 방법을 희소 표현(sparse representation)이라고 한다.  \r\n",
    " 그러니까 원-핫 벡터는 희소 벡터(sparse vector)다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import torch\r\n",
    "# 원-핫 벡터 생성\r\n",
    "dog = torch.FloatTensor([1, 0, 0, 0, 0])\r\n",
    "cat = torch.FloatTensor([0, 1, 0, 0, 0])\r\n",
    "computer = torch.FloatTensor([0, 0, 1, 0, 0])\r\n",
    "netbook = torch.FloatTensor([0, 0, 0, 1, 0])\r\n",
    "book = torch.FloatTensor([0, 0, 0, 0, 1])\r\n",
    "\r\n",
    "print(torch.cosine_similarity(dog, cat, dim=0))\r\n",
    "print(torch.cosine_similarity(cat, computer, dim=0))\r\n",
    "print(torch.cosine_similarity(computer, netbook, dim=0))\r\n",
    "print(torch.cosine_similarity(netbook, book, dim=0))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(0.)\n",
      "tensor(0.)\n",
      "tensor(0.)\n",
      "tensor(0.)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 원-핫 벡터로는 단어 간 의미적 유사도를 반영할 수 없다!!!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. 밀집 표현(Dense Representation)\r\n",
    "* 희소 표현과 반대되는 표현\r\n",
    "* 사용자가 설정한 값으로 모든 단어의 벡터 표현의 차원을 맞춥니다.   \r\n",
    "또한, 이 과정에서 더 이상 0과 1만 가진 값이 아니라 실수값을 가지게 됩니다.   \r\n",
    "\r\n",
    "  \r\n",
    "    \r\n",
    "Ex) 강아지 = [ 0 0 0 0 1 0 0 0 0 0 0 0 ... 중략 ... 0] # 이 때 1 뒤의 0의 수는 9995개. 차원은 10,000  \r\n",
    "  \r\n",
    "밀집 표현의 차원을 128로 설정한다면?  \r\n",
    "  \r\n",
    "Ex) 강아지 = [0.2 1.8 1.1 -2.1 1.1 2.8 ... 중략 ...] # 이 벡터의 차원은 128"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. 워드 임베딩(Word Embedding)\r\n",
    "* 단어를 밀집 벡터(dense vector)의 형태로 표현하는 방법\r\n",
    "* 이 밀집 벡터를 워드 임베딩 과정을 통해 나온 결과라고 하여 임베딩 벡터(embedding vector)\r\n",
    "* LSA, Word2Vec, FastText, Glove  \r\n",
    "  \r\n",
    "파이토치에서 제공하는 도구인 nn.embedding()는 앞서 언급한 방법들을 사용하지는 않지만, 단어를 랜덤한 값을 가지는 밀집 벡터로 변환한 뒤에, 인공 신경망의 가중치를 학습하는 것과 같은 방식으로 단어 벡터를 학습하는 방법을 사용합니다. "
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('chch': conda)"
  },
  "interpreter": {
   "hash": "a64de8b745ab094eb3381810b1d090f0053b4977cc21c07d5a367fb401258f96"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}