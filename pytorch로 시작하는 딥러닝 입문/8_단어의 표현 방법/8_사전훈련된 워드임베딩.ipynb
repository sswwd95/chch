{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 1. IMDB 리뷰 데이터를 훈련 데이터로 사용하기"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "cmd에서 conda install -c pytorch torchtext로 설치"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "from torchtext.legacy import data, datasets\r\n",
    "TEXT = data.Field(sequential=True, batch_first=True, lower=True)\r\n",
    "LABEL = data.Field(sequential=False, batch_first=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "trainset, testset = datasets.IMDB.splits(TEXT, LABEL, root = 'A:\\chchdata\\data')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "downloading aclImdb_v1.tar.gz\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "A:\\chchdata\\data\\imdb\\aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:56<00:00, 1.48MB/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "print('훈련 데이터의 크기 : {}' .format(len(trainset)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "훈련 데이터의 크기 : 25000\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "print(vars(trainset[1]))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'text': ['homelessness', '(or', 'houselessness', 'as', 'george', 'carlin', 'stated)', 'has', 'been', 'an', 'issue', 'for', 'years', 'but', 'never', 'a', 'plan', 'to', 'help', 'those', 'on', 'the', 'street', 'that', 'were', 'once', 'considered', 'human', 'who', 'did', 'everything', 'from', 'going', 'to', 'school,', 'work,', 'or', 'vote', 'for', 'the', 'matter.', 'most', 'people', 'think', 'of', 'the', 'homeless', 'as', 'just', 'a', 'lost', 'cause', 'while', 'worrying', 'about', 'things', 'such', 'as', 'racism,', 'the', 'war', 'on', 'iraq,', 'pressuring', 'kids', 'to', 'succeed,', 'technology,', 'the', 'elections,', 'inflation,', 'or', 'worrying', 'if', \"they'll\", 'be', 'next', 'to', 'end', 'up', 'on', 'the', 'streets.<br', '/><br', '/>but', 'what', 'if', 'you', 'were', 'given', 'a', 'bet', 'to', 'live', 'on', 'the', 'streets', 'for', 'a', 'month', 'without', 'the', 'luxuries', 'you', 'once', 'had', 'from', 'a', 'home,', 'the', 'entertainment', 'sets,', 'a', 'bathroom,', 'pictures', 'on', 'the', 'wall,', 'a', 'computer,', 'and', 'everything', 'you', 'once', 'treasure', 'to', 'see', 'what', \"it's\", 'like', 'to', 'be', 'homeless?', 'that', 'is', 'goddard', \"bolt's\", 'lesson.<br', '/><br', '/>mel', 'brooks', '(who', 'directs)', 'who', 'stars', 'as', 'bolt', 'plays', 'a', 'rich', 'man', 'who', 'has', 'everything', 'in', 'the', 'world', 'until', 'deciding', 'to', 'make', 'a', 'bet', 'with', 'a', 'sissy', 'rival', '(jeffery', 'tambor)', 'to', 'see', 'if', 'he', 'can', 'live', 'in', 'the', 'streets', 'for', 'thirty', 'days', 'without', 'the', 'luxuries;', 'if', 'bolt', 'succeeds,', 'he', 'can', 'do', 'what', 'he', 'wants', 'with', 'a', 'future', 'project', 'of', 'making', 'more', 'buildings.', 'the', \"bet's\", 'on', 'where', 'bolt', 'is', 'thrown', 'on', 'the', 'street', 'with', 'a', 'bracelet', 'on', 'his', 'leg', 'to', 'monitor', 'his', 'every', 'move', 'where', 'he', \"can't\", 'step', 'off', 'the', 'sidewalk.', \"he's\", 'given', 'the', 'nickname', 'pepto', 'by', 'a', 'vagrant', 'after', \"it's\", 'written', 'on', 'his', 'forehead', 'where', 'bolt', 'meets', 'other', 'characters', 'including', 'a', 'woman', 'by', 'the', 'name', 'of', 'molly', '(lesley', 'ann', 'warren)', 'an', 'ex-dancer', 'who', 'got', 'divorce', 'before', 'losing', 'her', 'home,', 'and', 'her', 'pals', 'sailor', '(howard', 'morris)', 'and', 'fumes', '(teddy', 'wilson)', 'who', 'are', 'already', 'used', 'to', 'the', 'streets.', \"they're\", 'survivors.', 'bolt', \"isn't.\", \"he's\", 'not', 'used', 'to', 'reaching', 'mutual', 'agreements', 'like', 'he', 'once', 'did', 'when', 'being', 'rich', 'where', \"it's\", 'fight', 'or', 'flight,', 'kill', 'or', 'be', 'killed.<br', '/><br', '/>while', 'the', 'love', 'connection', 'between', 'molly', 'and', 'bolt', \"wasn't\", 'necessary', 'to', 'plot,', 'i', 'found', '\"life', 'stinks\"', 'to', 'be', 'one', 'of', 'mel', \"brooks'\", 'observant', 'films', 'where', 'prior', 'to', 'being', 'a', 'comedy,', 'it', 'shows', 'a', 'tender', 'side', 'compared', 'to', 'his', 'slapstick', 'work', 'such', 'as', 'blazing', 'saddles,', 'young', 'frankenstein,', 'or', 'spaceballs', 'for', 'the', 'matter,', 'to', 'show', 'what', \"it's\", 'like', 'having', 'something', 'valuable', 'before', 'losing', 'it', 'the', 'next', 'day', 'or', 'on', 'the', 'other', 'hand', 'making', 'a', 'stupid', 'bet', 'like', 'all', 'rich', 'people', 'do', 'when', 'they', \"don't\", 'know', 'what', 'to', 'do', 'with', 'their', 'money.', 'maybe', 'they', 'should', 'give', 'it', 'to', 'the', 'homeless', 'instead', 'of', 'using', 'it', 'like', 'monopoly', 'money.<br', '/><br', '/>or', 'maybe', 'this', 'film', 'will', 'inspire', 'you', 'to', 'help', 'others.'], 'label': 'pos'}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "from gensim.models import KeyedVectors\r\n",
    "word2vec_model = KeyedVectors.load_word2vec_format('eng_w2v')\r\n",
    "print(word2vec_model['this']) # 영어 단어 'this'의 임베딩 벡터값 출력"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[-1.0787851  -1.4595797  -1.6951938  -1.0677129  -0.8161876   1.3518329\n",
      " -0.51005715  0.42380157 -0.929293    0.71384245  0.55134183  1.0590829\n",
      " -1.0150629  -0.03434338 -0.5499928   0.62797153  1.6568526   1.9338495\n",
      "  0.8115232   0.00942545  0.22666831  1.0393754   1.7679064   1.3355246\n",
      "  1.2987968  -1.9730322  -0.95621836  0.08263141 -0.30471653 -0.26669192\n",
      "  1.6205046   0.5179874   0.42467242 -2.4076445   1.0491213  -1.2095188\n",
      " -0.7935057  -0.2901258   0.945805   -1.8697681  -0.47169968 -2.4670858\n",
      " -1.1096776  -0.74332905  0.02778058 -2.0888884  -1.3114471   1.4873964\n",
      "  1.5078093  -2.0408823  -1.0439715   0.8034587   0.1026743   0.07208035\n",
      " -0.21107897 -0.66714126  0.29496312  0.01941346 -0.6947807   1.7927777\n",
      " -0.3508396   1.190273    0.70112103 -0.37626263 -2.3530893  -0.46349883\n",
      " -1.1945626   0.2383219  -0.23746352 -0.07986329  0.03753169  0.5586949\n",
      " -1.7013954   2.0230076  -0.13451375 -0.03937571 -0.4343432  -1.0143398\n",
      "  0.33852136  0.1052811  -0.2808158  -0.47010344  1.1724986   0.48922455\n",
      "  1.2253138  -1.6259937  -0.69003075 -0.18363291 -1.8120416  -0.2895791\n",
      " -0.04391266  0.14041948  1.203725   -0.73124146  1.5237558   0.04164498\n",
      "  1.1100339   1.8582475   2.0867946   0.6604321 ]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. 사전 훈련된 Word2Vec을 초기 임베딩으로 사용하기"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "from torchtext.vocab import Vectors"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "vectors = Vectors(name=\"eng_w2v\") # 사전 훈련된 Word2Vec 모델을 vectors에 저장"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 0/21613 [00:00<?, ?it/s]WARNING:torchtext.vocab:Skipping token b'21613' with 1-dimensional vector [b'100']; likely a header\n",
      "100%|██████████| 21613/21613 [00:00<00:00, 27360.28it/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "TEXT.build_vocab(trainset, vectors=vectors, max_size=10000, min_freq=10) # Word2Vec 모델을 임베딩 벡터값으로 초기화"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "* max_size와 min_freq는 몇 개의 단어들만을 가지고 단어 집합을 생성할 것인지를 정합니다. max_size는 단어 집합의 크기를 제한하고, min_freq=10은 등장 빈도수가 10번 이상인 단어만 허용하는 것한다는 의미입니다.  \r\n",
    "* vectors=vectors는 만들어진 단어 집합의 각 단어의 임베딩 벡터값으로 env_w2v에 저장되어져 있던 임베딩 벡터값들로 초기화합니다"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "# print(TEXT.vocab.stoi)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "print(TEXT.vocab.vectors[0]) # <unk>의 임베딩 벡터값"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0.])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "print(TEXT.vocab.vectors[1]) # <pad>의 임베딩 벡터값"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0.])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "unk와 pad의 임베딩 백터값이 0으로 초기화된다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "embedding_layer = nn.Embedding.from_pretrained(TEXT.vocab.vectors, freeze=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "print(embedding_layer(torch.LongTensor([10]))) # 단어 this의 임베딩 벡터값"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[-1.0788, -1.4596, -1.6952, -1.0677, -0.8162,  1.3518, -0.5101,  0.4238,\n",
      "         -0.9293,  0.7138,  0.5513,  1.0591, -1.0151, -0.0343, -0.5500,  0.6280,\n",
      "          1.6569,  1.9338,  0.8115,  0.0094,  0.2267,  1.0394,  1.7679,  1.3355,\n",
      "          1.2988, -1.9730, -0.9562,  0.0826, -0.3047, -0.2667,  1.6205,  0.5180,\n",
      "          0.4247, -2.4076,  1.0491, -1.2095, -0.7935, -0.2901,  0.9458, -1.8698,\n",
      "         -0.4717, -2.4671, -1.1097, -0.7433,  0.0278, -2.0889, -1.3114,  1.4874,\n",
      "          1.5078, -2.0409, -1.0440,  0.8035,  0.1027,  0.0721, -0.2111, -0.6671,\n",
      "          0.2950,  0.0194, -0.6948,  1.7928, -0.3508,  1.1903,  0.7011, -0.3763,\n",
      "         -2.3531, -0.4635, -1.1946,  0.2383, -0.2375, -0.0799,  0.0375,  0.5587,\n",
      "         -1.7014,  2.0230, -0.1345, -0.0394, -0.4343, -1.0143,  0.3385,  0.1053,\n",
      "         -0.2808, -0.4701,  1.1725,  0.4892,  1.2253, -1.6260, -0.6900, -0.1836,\n",
      "         -1.8120, -0.2896, -0.0439,  0.1404,  1.2037, -0.7312,  1.5238,  0.0416,\n",
      "          1.1100,  1.8582,  2.0868,  0.6604]], grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 임베딩 벡터 리스트의 일부\r\n",
    "* fasttext.en.300d\r\n",
    "* fasttext.simple.300d\r\n",
    "* glove.42B.300d\r\n",
    "* glove.840B.300d\r\n",
    "* glove.twitter.27B.25d\r\n",
    "* glove.twitter.27B.50d\r\n",
    "* glove.twitter.27B.100d\r\n",
    "* glove.twitter.27B.200d\r\n",
    "* glove.6B.50d\r\n",
    "* glove.6B.100d\r\n",
    "* glove.6B.200d\r\n",
    "* glove.6B.300d <= 이걸 사용해볼 겁니다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### IMDB 리뷰 데이터에 존재하는 단어들을 토치텍스트가 제공하는 사전 훈련된 임베딩 벡터들의 값으로 초기화해봅시다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "from torchtext.vocab import GloVe\r\n",
    "TEXT.build_vocab(trainset, vectors=GloVe(name='6B', dim=300), max_size=10000, min_freq=10)\r\n",
    "LABEL.build_vocab(trainset)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      ".vector_cache\\glove.6B.zip: 862MB [11:04, 1.30MB/s]                           \n",
      "100%|█████████▉| 399999/400000 [00:33<00:00, 11905.33it/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "# print(TEXT.vocab.stoi)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "print('임베딩 벡터의 개수와 차원 : {} '.format(TEXT.vocab.vectors.shape))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "임베딩 벡터의 개수와 차원 : torch.Size([10002, 300]) \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('chch': conda)"
  },
  "interpreter": {
   "hash": "a64de8b745ab094eb3381810b1d090f0053b4977cc21c07d5a367fb401258f96"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}