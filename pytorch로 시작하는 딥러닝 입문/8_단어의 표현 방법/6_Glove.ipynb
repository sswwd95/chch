{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "* 카운트 기반과 예측 기반을 모두 사용하는 방법론\r\n",
    "* 2014년에 미국 스탠포드대학에서 개발한 단어 임베딩 방법론\r\n",
    "* 카운트 기반의 LSA(Latent Semantic Analysis)와 예측 기반의 Word2Vec의 단점을 지적하며 이를 보완한다는 목적으로 나왔고, 실제로도 Word2Vec만큼 뛰어난 성능\r\n",
    "* 이 두 가지 전부를 사용해보고 성능이 더 좋은 것을 사용하는 것이 바람직하다.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. 기존 방법론에 대한 비판"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LSA\r\n",
    "* LSA는 각 단어의 빈도수를 카운트 한 행렬이라는 전체적인 통계 정보를 입력으로 받아 차원을 축소(Truncated SVD)하여 잠재된 의미를 끌어내는 방법론\r\n",
    "* 운트 기반으로 코퍼스의 전체적인 통계 정보를 고려\r\n",
    "* 단어 의미의 유추 작업(Analogy task)에는 성능이 떨어진다.\r\n",
    "\r\n",
    "## Word2Vec\r\n",
    "* Word2Vec는 실제값과 예측값에 대한 오차를 손실 함수를 통해 줄여나가며 학습하는 예측 기반의 방법론\r\n",
    "* 단어 간 유추 작업에는 LSA보다 뛰어남\r\n",
    "* 임베딩 벡터가 윈도우 크기 내에서만 주변 단어를 고려하기 때문에 코퍼스의 전체적인 통계 정보를 반영하지 못한다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. 윈도우 기반 동시 등장 행렬(Window based Co-occurrence Matrix) \r\n",
    "단어의 동시 등장 행렬은 행과 열을 전체 단어 집합의 단어들로 구성하고, i 단어의 윈도우 크기(Window Size) 내에서 k 단어가 등장한 횟수를 i행 k열에 기재한 행렬을 말합니다. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- I like deep learning\r\n",
    "- I like NLP\r\n",
    "- I enjoy flying\r\n",
    "\r\n",
    "위의 예시에서 윈도우 사이즈 1이라고 한다면, 아래의 행렬과 같다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\r\n",
    "|카운트\t|I\t|like\t|enjoy|\tdeep|\tlearning|\tNLP\t|flying|\r\n",
    "|----|---|---|---|---|---|---|---|\r\n",
    "|I\t|0\t|2\t|1|\t0|\t0|\t0|\t0|\r\n",
    "|like\t|2\t|0|\t0|\t1|\t0|\t1|\t0|\r\n",
    "|enjoy\t|1\t|0|\t0|\t0|\t0|\t0|\t1|\r\n",
    "|deep\t|0\t|1|\t0|\t0|\t1|\t0|\t0|\r\n",
    "|learning|0|0|\t0|\t1|\t0|\t0|\t0|\r\n",
    "|NLP\t|0\t|1|\t0|\t0|\t0|\t0|\t0|\r\n",
    "|flying\t|0\t|0|\t1|\t0\t|0\t|0\t|0|\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "'I'에서 윈도우 사이즈 1이니까 바로 옆의 단어만 참고하여 <u>llike</u>는 'I'기준으로 두번 등장하고 <u>enjoy</u>는 1번 등장한다. \r\n",
    "* **I** <u>like</u> deep learning\r\n",
    "* **I** <u>like</u> NLP\r\n",
    "* **I** <u>enjoy</u> flying"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "위 행렬은 행렬을 전치(Transpose)해도 동일한 행렬이 된다는 특징  \r\n",
    "-> i 단어의 윈도우 크기 내에서 k 단어가 등장한 빈도는 반대로 k 단어의 윈도우 크기 내에서 i 단어가 등장한 빈도와 동일하기 때문"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. 동시 등장 확률(Co-occurrence Probability)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "동시 등장 확률 $P(k\\ |\\ i)$는 동시 등장 행렬로부터 특정 단어 i의 전체 등장 횟수를 카운트하고, 특정 단어 i가 등장했을 때 어떤 단어 k가 등장한 횟수를 카운트하여 계산한 조건부 확률"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "i를 중심 단어(Center Word), k를 주변 단어(Context Word)라고 했을 때, 위에서 배운 동시 등장 행렬에서 중심 단어 i의 행의 모든 값을 더한 값을 분모로 하고 i행 k열의 값을 분자로 한 값  \r\n",
    "* i를 ice, steam으로 정하고 k를 water, fashion이라고 했을때 P(solid l ice) / P(solid l steam)를 계산하면두 단어 모두와 동시 등장하는 경우가 많으면 1에 가까운 값이 나오고, 동시 등자하는 경우가 모두 적어도 1에 가까운 값이 나온다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# GloVe : 임베딩 된 중심 단어와 주변 단어 벡터의 내적이 전체 코퍼스에서의 동시 등장 확률이 되도록 만드는 것"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. 손실 함수(Loss function)\r\n",
    "GloVe는 아래와 같은 관계를 가지도록 임베딩 벡터를 설계한다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "$dot\\ product(w_{i}\\ \\tilde{w_{k}}) \\approx\\ log\\ P(k\\ |\\ i) = log\\ P_{ik}$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "최종적으로 아래와 같은 손실 함수를 얻어낼 수 있다.  \r\n",
    "$Loss\\ function = \\sum_{m, n=1}^{V}\\ f(X_{mn})(w_{m}^{T}\\tilde{w_{n}} + b_{m} + \\tilde{b_{n}} - logX_{mn})^{2}$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "!pip install glove_python_binary"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting glove_python_binary\n",
      "  Downloading glove_python_binary-0.2.0-cp38-cp38-win_amd64.whl (244 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\sswwd\\anaconda3\\envs\\chch\\lib\\site-packages (from glove_python_binary) (1.21.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\sswwd\\anaconda3\\envs\\chch\\lib\\site-packages (from glove_python_binary) (1.7.1)\n",
      "Installing collected packages: glove-python-binary\n",
      "Successfully installed glove-python-binary-0.2.0\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "import nltk\r\n",
    "nltk.download('punkt')\r\n",
    "import urllib.request\r\n",
    "import zipfile\r\n",
    "from lxml import etree\r\n",
    "import re\r\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sswwd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "targetXML=open('ted_en-20160408.xml', 'r', encoding='UTF8')\r\n",
    "# 저자의 경우 윈도우 바탕화면에서 작업하여서 'C:\\Users\\USER\\Desktop\\ted_en-20160408.xml'이 해당 파일의 경로.  \r\n",
    "target_text = etree.parse(targetXML)\r\n",
    "parse_text = '\\n'.join(target_text.xpath('//content/text()'))\r\n",
    "# xml 파일로부터 <content>와 </content> 사이의 내용만 가져온다.\r\n",
    "\r\n",
    "content_text = re.sub(r'\\([^)]*\\)', '', parse_text)\r\n",
    "# 정규 표현식의 sub 모듈을 통해 content 중간에 등장하는 (Audio), (Laughter) 등의 배경음 부분을 제거.\r\n",
    "# 해당 코드는 괄호로 구성된 내용을 제거.\r\n",
    "\r\n",
    "sent_text = sent_tokenize(content_text)\r\n",
    "# 입력 코퍼스에 대해서 NLTK를 이용하여 문장 토큰화를 수행.\r\n",
    "\r\n",
    "normalized_text = []\r\n",
    "for string in sent_text:\r\n",
    "     tokens = re.sub(r\"[^a-z0-9]+\", \" \", string.lower())\r\n",
    "     normalized_text.append(tokens)\r\n",
    "# 각 문장에 대해서 구두점을 제거하고, 대문자를 소문자로 변환.\r\n",
    "\r\n",
    "result = []\r\n",
    "result = [word_tokenize(sentence) for sentence in normalized_text]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "from glove import Corpus, Glove\r\n",
    "corpus = Corpus()\r\n",
    "corpus.fit(result,window=5)\r\n",
    "\r\n",
    "# 경사하강법 학습률 0.05, 아웃풋 벡터의 차원 100\r\n",
    "glove = Glove(no_components=100, learning_rate=0.05)\r\n",
    "# 쓰레드 개수는 4개, 훈련 횟수는 20번, verbose (설명) True\r\n",
    "glove.fit(corpus.matrix, epochs=20, no_threads=10, verbose=True)\r\n",
    "# 유사도 검색을 위한 행렬의 index 정보 입력\r\n",
    "glove.add_dictionary(corpus.dictionary)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Performing 20 training epochs with 10 threads\n",
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n",
      "Epoch 17\n",
      "Epoch 18\n",
      "Epoch 19\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "model_result1 = glove.most_similar(\"man\")\r\n",
    "print(model_result1)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('woman', 0.9639851588239543), ('guy', 0.8726794502997316), ('girl', 0.863441713905972), ('young', 0.8519259457897719)]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## w2v 결과\r\n",
    "[('woman', 0.8452662825584412), ('guy', 0.7914076447486877), ('lady', 0.7580448389053345), ('boy', 0.7566693425178528), ('girl', 0.7327544093132019), ('gentleman', 0.7280296087265015), ('soldier', 0.723828911781311), ('poet', 0.6915566921234131), ('kid', 0.6702145338058472), ('surgeon', 0.6474255919456482)]"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "model_result3=glove.most_similar(\"university\")\r\n",
    "print(model_result3)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('harvard', 0.8820422020162428), ('mit', 0.8432864536485946), ('stanford', 0.8321941194805292), ('cambridge', 0.8303369834037834)]\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('chch': conda)"
  },
  "interpreter": {
   "hash": "a64de8b745ab094eb3381810b1d090f0053b4977cc21c07d5a367fb401258f96"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}